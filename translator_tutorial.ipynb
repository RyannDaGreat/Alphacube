{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8bbcef5-4358-46b6-9c19-155c9bace134",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98261340-20d7-48df-bbab-d9ca3119ce6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297479ca-2c18-4cb9-aab9-10ac37619b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import icecream\n",
    "sys.path.append('./translator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1930b7-b2eb-462f-a765-49bab7851885",
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "QDs4Im9WTQoy"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import rp\n",
    "from translator.trainer import MUNIT_Trainer as Trainer\n",
    "from translator.data import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bb7ce7-1dd5-4cb4-8d43-2ef4acb13d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaa85a3-b966-4ce5-8c1f-90e59f7e6b72",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Other Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be09caf-f560-481e-82f0-881dee44f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c206b5-ac5c-43af-8f62-800da87b35a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c90e85-0b30-463b-ab23-f465fc5b6678",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d8c826-a8c7-449d-ab9c-253907611853",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alphablock\n",
    "config_file       = './translator/configs/alphablock_without_ssim_256.yaml'\n",
    "image_folder_path = './datasets/alphacube/scenes/'\n",
    "# image_folder_path = './datasets/alphacube/anim_2/'\n",
    "# image_folder_path = './datasets/alphacube/anim_1/'\n",
    "image_folder_path = '/mnt/Noman/Ubuntu/CleanCode/Datasets/diff_rendering/alphabetcube_L/SyntheticData/Anim3/Renderings'\n",
    "checkpoint_folder = './translator/trained_models/outputs/alphablock_without_ssim_256/checkpoints'\n",
    "# checkpoint_folder = './translator/trained_models/outputs/alphablock_without_ssim_256/checkpoints/old_checkpoints/v0.0.7'\n",
    "# checkpoint_folder = './translator/save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e54e4-7d18-445b-937f-4222b10830b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sunkist\n",
    "config_file       = './translator/configs/config.yaml'\n",
    "image_folder_path = '/mnt/Noman/Ubuntu/CleanCode/Datasets/diff_rendering/sunkist/synthetic/RenderingsAnim'\n",
    "checkpoint_folder = './translator/trained_models/outputs/config/checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1d7724-789a-498d-9e88-cc027a262687",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = rp.load_yaml_file(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd994fdf-d690-4dde-99aa-dc03da8cd630",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=Trainer(config, trainable=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c516e9-ffba-4de4-b039-68a7eb5a1e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.resume(checkpoint_folder, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d71943-7f6d-47d9-8711-669e7c2a3c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.display_image(rp.tiled_images(rp.as_numpy_images(trainer.texture_pack())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e207e22-4cd7-48b3-be2a-378b52805172",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TODO: Make the Trainer save the neural texture once, so during inference we don't need to run it over and over again (it's redundant to do that seeing as the texture never changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dafbc9-4b72-40b7-8a6f-0ab673649c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = {}\n",
    "aug[\"new_size_min\"] = config[\"new_size_min_a\"]\n",
    "aug[\"new_size_max\"] = config[\"new_size_max_a\"]\n",
    "aug[\"output_size\" ] = (-1,-1) #Meaningless when skip_crop = True\n",
    "# aug[\"output_size\" ] = (320,320) #Meaningless when skip_crop = True\n",
    "image_folder = ImageFolder(root=image_folder_path, precise=True, augmentation=aug)\n",
    "image_folder.skip_crop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d859811e-840c-451f-b92e-13afa7c12aae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9097fa8-e11e-4f13-8489-33a3d779ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "o=rp.random_element(image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5be34ae-f035-4a8b-b734-c8d363e7867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, height, width = o.shape\n",
    "icecream.ic(height,width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7b9319-a9b0-4e01-8948-a21d1e28182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.display_image(rp.as_numpy_image(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001f9d49-5254-4b9d-bd4b-d9860acb8c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h=o[None].to(device)\n",
    "i=trainer.sample_a2b(h)\n",
    "\n",
    "\n",
    "rp.display_image(rp.as_numpy_image(h[0]))\n",
    "rp.display_image(rp.as_numpy_image(i[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a04f55-2448-4b93-8009-8878a72b66c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=trainer.sample(h,h)[:7]\n",
    "for x in i:\n",
    "    rp.display_image(rp.as_numpy_image(x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7319ec8f-d46b-4ce8-961c-6b672285267b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d469bda-215d-43e6-8eac-4194afb81681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(image):\n",
    "    #Input image is a UV-L Scene\n",
    "    assert rp.is_image(image)\n",
    "    image = rp.as_rgb_image  (image)\n",
    "    image = rp.as_float_image(image)\n",
    "    \n",
    "    image = rp.as_torch_image(image)[None] #BCHW\n",
    "    image = image.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = trainer.sample_a2b(image)\n",
    "    output = output[0]\n",
    "    output = rp.as_numpy_image(output)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506427c2-7f09-4c85-9846-054ac333224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = rp.load_images(image_folder_path, show_progress=True)\n",
    "clear_output()\n",
    "\n",
    "images = [rp.cv_resize_image(image, (height, width), 'nearest') for image in images]\n",
    "uvl_scenes=images.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c29c1d-651d-49a0-aba8-125edaa6215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "bad_indices=[i for i,x in enumerate(images) if not np.any(x)]\n",
    "for i in bad_indices:\n",
    "    del images[i]\n",
    "    del uvl_scenes[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc203b5-6f3c-4cfe-9f05-ef66a92f1554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_eta = rp.eta(len(images))\n",
    "translations = []\n",
    "for index,image in enumerate(images):\n",
    "    display_eta(index)\n",
    "    with torch.no_grad():\n",
    "        translations.append(translate(image))\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d72e95-099b-4275-b7ab-4dc74bf979e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please be patient, this step can take a long time to complete (up to 5 minutes)\n",
    "#This is because it's extremely inefficient lol. All images in this animation are being converted into base-64 in html\n",
    "# rp.display_image_slideshow(translations[::5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd05c1dd-a26a-4439-81bb-5777166df074",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_path = 'untracked/uvl_animation.mp4'\n",
    "rp.save_video_mp4([rp.as_rgb_image(rp.as_byte_image(x)) for x in images],video_path,video_bitrate='medium')\n",
    "clear_output()\n",
    "# translations=[rp.as_byte_image(x) for x in translations]\n",
    "# translations=rp.as_numpy_array(translations)\n",
    "# write_mp4(video_path, translations)\n",
    "\n",
    "from IPython.display import Video\n",
    "icecream.ic(video_path)\n",
    "Video(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634462cd-d43d-436a-815f-b03a434a12f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'untracked/animation.mp4'\n",
    "rp.save_video_mp4(translations,'untracked/animation.mp4', video_bitrate='large')\n",
    "clear_output()\n",
    "# translations=[rp.as_byte_image(x) for x in translations]\n",
    "# translations=rp.as_numpy_array(translations)\n",
    "# write_mp4(video_path, translations)\n",
    "\n",
    "from IPython.display import Video\n",
    "icecream.ic(video_path)\n",
    "Video(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee28c95-aa1d-4e31-8175-9cd1a473d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.unprojector import unproject_translations\n",
    "from source.unprojector import unproject_translations_individually\n",
    "from source.unprojector import combine_individual_unprojections\n",
    "from source.scene_reader import extract_scene_uvs_and_scene_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7644707-0732-4498-96a3-65b2b96010ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_uvs, scene_labels = extract_scene_uvs_and_scene_labels(rp.as_torch_images(rp.as_numpy_array(uvl_scenes)),[0,255])\n",
    "translations=rp.as_numpy_array(translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5999c102-31cc-4109-9cd9-43c20f5b084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_trans = rp.as_torch_images(rp.as_numpy_array(translations))\n",
    "\n",
    "height=min(scene_trans.shape[2],scene_uvs.shape[2])\n",
    "width =min(scene_trans.shape[3],scene_uvs.shape[3])\n",
    "scene_trans=scene_trans[:,:,:height,:width]\n",
    "scene_uvs=scene_uvs[:,:,:height,:width]\n",
    "scene_labels=scene_labels[:,:height,:width]\n",
    "\n",
    "\n",
    "icecream.ic(scene_trans.shape, scene_trans.dtype, scene_trans.device);\n",
    "icecream.ic(scene_trans.shape, scene_uvs.shape, scene_labels.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976caf41-d983-4354-b6a2-86f065be0b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "recovery_resolution=1024\n",
    "with torch.no_grad():\n",
    "    recovered_textures, recovered_weights = unproject_translations_individually(scene_trans[::4],\n",
    "                                                                   scene_uvs[::4],  scene_labels[::4],\n",
    "                                                                   num_labels   =3                                        ,\n",
    "                                                                   output_height=recovery_resolution                      ,\n",
    "                                                                   output_width =recovery_resolution                      )\n",
    "    \n",
    "recovered_textures, recovered_weights =combine_individual_unprojections(recovered_textures, recovered_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e449db-64aa-4107-a8bd-9a727e24d0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recovered_textures=(recovered_weights[:,:,None]*recovered_textures).sum(0)/(recovered_weights.sum(0)+.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc68c55-c648-440b-9849-78bf88f2e74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.display_image(rp.tiled_images(rp.as_numpy_images(recovered_textures)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df34f7f4-18d8-4fc1-b1c4-3e98daf15997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.projector import project_textures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167d1f53-d25f-4670-bf1d-9c6e594f2355",
   "metadata": {},
   "outputs": [],
   "source": [
    "reprojections=project_textures(scene_uvs,scene_labels,recovered_textures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4a07ef-9fd2-464b-94fa-e8181d409e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "rp.save_video(rp.as_numpy_images(reprojections),'untracked/reproject.mp4')\n",
    "clear_output()\n",
    "Video('untracked/reproject.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5430d663-bbcb-4cdd-9724-eef896260aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "shadows = rp.as_numpy_array(translations)-rp.as_numpy_images(reprojections)\n",
    "rp.save_video(.5+rp.as_numpy_images(shadows),'untracked/shadows.mp4')\n",
    "clear_output()\n",
    "Video('untracked/shadows.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830f1df3-1783-49cb-ade9-02d192c5fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "shadows =rp.as_numpy_array(translations)-rp.as_numpy_images(reprojections)\n",
    "# shadows = [rp.cv_gauss_blur(x,3) for x in shadows]\n",
    "# shadows = rp.as_numpy_array(shadows)\n",
    "# shadows_amp = np.abs(shadows)\n",
    "# shadows_sign = np.sign(shadows)\n",
    "# shadows = [rp.cv_dilate(rp.cv_erode(x,3),3) for x in shadows_amp]\n",
    "# shadows = shadows * shadows_sign\n",
    "shadows = rp.as_numpy_array(shadows)\n",
    "rp.save_video(.5+rp.as_numpy_images(shadows),'untracked/shadows.mp4')\n",
    "clear_output()\n",
    "Video('untracked/shadows.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29404a7-9dc0-49fd-b132-d2bda8fd1496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "redone = shadows*3+rp.as_numpy_images(reprojections)\n",
    "rp.save_video(rp.as_numpy_images(redone),'untracked/redone.mp4')\n",
    "clear_output()\n",
    "Video('untracked/redone.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a79d74-c101-46f5-9e6f-7bc7291c54b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
