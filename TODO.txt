Organize this TODO

For presentation:
  Create animations of evolution over time. Note how the new one prefers circular rings at first, and the weird artifacts.
  Create animations of the cubes moving to SHOW temporal consistency. Chart their (recovered) textures throughout the animation as well.
    Do this all in a jupyter notebook.
    Use git LFS to store the animation files and ipynb's containing said animations. Animations will be saved as a series of JPG's.
  Put the images in my downloads folder into th presentaiotn

Debug what I did to make vim start up so slowly..........oooofffff.......set up a git repo for vimrc......

Save textures as part of the training process...

NOTES:
  As we see in version 0.0.3, a large batch size is very slow...but it helps. It makes it much better. (Note large = 5, as opposed to 2. 1 is useless: there is no view consistency loss with a batch size of 1)
  The texture multiplier has a big effect! (Though to be fair it's confounded with fourier). I'm not sure why it works exactly...but I think it's because of regularization? Anyway, when it's .1 I see small changes to the floor mainly...but when it's .5....I see the L shape on the cube!!!! It's super active!
  A big difference between mine and theirs: theirs takes 6 channels as an input...but mine only takes 3 xD I want to see if I can keep it that way...it would make it great for the simulators!!! (Residual textures FTW!)
    If transformers can do residual stuff instead of concatenating because of frequency differences...why can't I? I have a theory on why that worked, as I told letitia (coffe bean channel), and I was right I think? In either case, it works! 
  A big difference between tex and no tex: the blurryness of the table. It doesn't shift!
    If we ever use mobile robots with multi rooms, many images will share 0 common UV's. In this case we have to choose the batch less randomly: choose images that overlap
  Setting the texture multiplier from .1 to .5, then from .5 to 5 had a big impact on the evolution of the algo. .5 was better than .1 (it created an L shape and drew on the cube, unlike .1 which didn't draw on the cube). But 5, learned much faster that the top of the cube is red and white than .5 did (epoch 2000 vs 10000 or so lol)

The gen_opt and dis_opt both have schedulers. Should the texture optimizer? Would that make it better?


The table blur: Why horizontal? It comes from inconsistencies from the cropping process: the images are only shifted left and right because of the rescaling and the fact that the input images are rectantular. Once we were able to know where it was (because of the textures) that problem dissapeared! This also means the textures helped!!

See if I can get away with smaller batch sizes? It might go from 6 hours to 3 hours....maybe....


TODO:
    The original code for lapro (in the archived things above this dir) is better.
    The Nerfed one, in particular, achieves *much* cleaner, smoother results than this one when used in vanilla mode.
        NOTE: This might have something to do with BILINEAR vs NEAREST (precise) in data.py? Note how the wooden background looks smoother too...
            ** precise is implemented right tho...
        NOTE: I think this happened when i started using .exr's?
    The view consistency loss helps bring it closer to that, but we really should merge the differences.
    Do some tests: when exactly did it start to look worse? (i'm not sure. maybe it's the new way we load data? We should find out...
        ...but unfortunately this takes time...looking at the pics can we predict whether it will be smooth or not from an early epoch?)

TODO:
    Create a python .rpy file to interface with the im2im: I want a simple input-output function that takes in the UVL maps (UV, Label) and outputs a translated image.
    From this, we can create animations, and create our own sample images for further testing etc.
    TODO:
        We need to simulate the augmentation scaling etc that this algorithm uses somehow, in a nice deterministic way...however the test images are made we should imitate that
        (if we want a smooth animation, the preprocessing has to be nonrandom)

I made the renderer in the gen but NOT in anything else, such as the translations.py or the discriminator trainer!
It HAS to be EVERYWHERE!
TODO:
    Make a more serious attempt. Don't rush this.
    We need image outputs for 6 channels. Why don't we start here? Have 6-channel inputs. Also, render the textures. Try all of that.
    Then, solve the NAN problem. Why are there NAN's?
   
