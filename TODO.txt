Organize this TODO

For presentation:
  Create animations of evolution over time. Note how the new one prefers circular rings at first, and the weird artifacts.
  Create animations of the cubes moving to SHOW temporal consistency. Chart their (recovered) textures throughout the animation as well.
    Do this all in a jupyter notebook.
    Use git LFS to store the animation files and ipynb's containing said animations. Animations will be saved as a series of JPG's.
  Put the images in my downloads folder into th presentaiotn

Debug what I did to make vim start up so slowly..........oooofffff.......set up a git repo for vimrc......

Save textures as part of the training process...

NOTES:
  As we see in version 0.0.3, a large batch size is very slow...but it helps. It makes it much better. (Note large = 5, as opposed to 2. 1 is useless: there is no view consistency loss with a batch size of 1)
  The texture multiplier has a big effect! (Though to be fair it's confounded with fourier). I'm not sure why it works exactly...but I think it's because of regularization? Anyway, when it's .1 I see small changes to the floor mainly...but when it's .5....I see the L shape on the cube!!!! It's super active!
  A big difference between mine and theirs: theirs takes 6 channels as an input...but mine only takes 3 xD I want to see if I can keep it that way...it would make it great for the simulators!!! (Residual textures FTW!)
    If transformers can do residual stuff instead of concatenating because of frequency differences...why can't I? I have a theory on why that worked, as I told letitia (coffe bean channel), and I was right I think? In either case, it works! 
  A big difference between tex and no tex: the blurryness of the table. It doesn't shift!
    If we ever use mobile robots with multi rooms, many images will share 0 common UV's. In this case we have to choose the batch less randomly: choose images that overlap
  Setting the texture multiplier from .1 to .5, then from .5 to 5 had a big impact on the evolution of the algo. .5 was better than .1 (it created an L shape and drew on the cube, unlike .1 which didn't draw on the cube). But 5, learned much faster that the top of the cube is red and white than .5 did (epoch 2000 vs 10000 or so lol)

The gen_opt and dis_opt both have schedulers. Should the texture optimizer? Would that make it better?


The table blur: Why horizontal? It comes from inconsistencies from the cropping process: the images are only shifted left and right because of the rescaling and the fact that the input images are rectantular. Once we were able to know where it was (because of the textures) that problem dissapeared! This also means the textures helped!!

See if I can get away with smaller batch sizes? It might go from 6 hours to 3 hours....maybe....


TODO:
    The original code for lapro (in the archived things above this dir) is better.
    The Nerfed one, in particular, achieves *much* cleaner, smoother results than this one when used in vanilla mode.
        NOTE: This might have something to do with BILINEAR vs NEAREST (precise) in data.py? Note how the wooden background looks smoother too...
            ** precise is implemented right tho...
        NOTE: I think this happened when i started using .exr's?
    The view consistency loss helps bring it closer to that, but we really should merge the differences.
    Do some tests: when exactly did it start to look worse? (i'm not sure. maybe it's the new way we load data? We should find out...
        ...but unfortunately this takes time...looking at the pics can we predict whether it will be smooth or not from an early epoch?)

TODO:
    Create a python .rpy file to interface with the im2im: I want a simple input-output function that takes in the UVL maps (UV, Label) and outputs a translated image.
    From this, we can create animations, and create our own sample images for further testing etc.
    TODO:
        We need to simulate the augmentation scaling etc that this algorithm uses somehow, in a nice deterministic way...however the test images are made we should imitate that
        (if we want a smooth animation, the preprocessing has to be nonrandom)

I made the renderer in the gen but NOT in anything else, such as the translations.py or the discriminator trainer!
It HAS to be EVERYWHERE!
TODO:
    Make a more serious attempt. Don't rush this.
    We need image outputs for 6 channels. Why don't we start here? Have 6-channel inputs. Also, render the textures. Try all of that.
    Then, solve the NAN problem. Why are there NAN's?
   
TO PRESENT:
    Why use fourier instead of pixels?
        Fourier means that when we have gaps between the UV positions in our translated scene (like we often do; see the unprojections in the demo - there are holes everywhere) that the parts in between are also evolved. This means the UV maps being good also has a positive effect on the results; and that the textures we generate are technically resolution independent (which is pretty cool)
        TLDR: it help it learn better from incomplete information

TODO:
    Come up with a strategy to choose batches that have common UV positions so we can train more efficiently. This will be super important later on when we have more complicated scenes; and ESCPECIALLY when we have mobile robotic scenes.

RYOO TODO:
    View consistency should be called texture consistncy
    EXPERIMENTS:
        We can exclude the uarm form the view and just do a bunch of close-your-eyes then manipilate of scenes to see if this method outperformes the baseline
    Neural texture could be called latent texture (this would break convention though)

TODO: Fibbing
    Try "Fibbing" textures: using the translations, we take advantage of the fact that a single translation is less blurry than an average of them. Instead of averaging them all together, we choose a tranlsation with maximal texture coverage for a particular label. Then we choose another translation that agrees with it most (prefereably agrees vacuously; meaning it has no common texture areas). Fill that in. No averaging: just writing in pixels to the recovered texture. Keep choosing the texture that agrees most with the current recovered texture. There are many possible solutions, so its nose grows like pinnochio: it's fibbing!
    
    
FURTHER EXPERIMENTS:
    I'm having difficulty increasing the resultion (becuase of vram limitations I had to crop the image etc). And I noticed that the texture doesnt always align with the translation's cube positions...what if we add another loss to make the translated texture similar to the translation, and apply it in both directions like QVAE?
    Use MSSIM in basic ipynb
    Use MSSIM here too
