NOTES:
  As we see in version 0.0.3, a large batch size is very slow...but it helps. It makes it much better. (Note large = 5, as opposed to 2. 1 is useless: there is no view consistency loss with a batch size of 1)


TODO:
    The original code for lapro (in the archived things above this dir) is better.
    The Nerfed one, in particular, achieves *much* cleaner, smoother results than this one when used in vanilla mode.
    The view consistency loss helps bring it closer to that, but we really should merge the differences.
    Do some tests: when exactly did it start to look worse? (i'm not sure. maybe it's the new way we load data? We should find out...
        ...but unfortunately this takes time...looking at the pics can we predict whether it will be smooth or not from an early epoch?)

TODO:
    Create a python .rpy file to interface with the im2im: I want a simple input-output function that takes in the UVL maps (UV, Label) and outputs a translated image.
    From this, we can create animations, and create our own sample images for further testing etc.
    TODO:
        We need to simulate the augmentation scaling etc that this algorithm uses somehow, in a nice deterministic way...however the test images are made we should imitate that
        (if we want a smooth animation, the preprocessing has to be nonrandom)

I made the renderer in the gen but NOT in anything else, such as the translations.py or the discriminator trainer!
It HAS to be EVERYWHERE!
TODO:
    Make a more serious attempt. Don't rush this.
    We need image outputs for 6 channels. Why don't we start here? Have 6-channel inputs. Also, render the textures. Try all of that.
    Then, solve the NAN problem. Why are there NAN's?
   
