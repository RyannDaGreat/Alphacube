VERY IMPORTANT TEST: ROBUSTNESS
    What if the camera moves? What if the lighting changes?
    What if we want a first person robot to push a block into a goal?
NEW TESTS:
    Robot jetson bot.
    We can do Jinghuan's experiment (multiview) with this! Can we try that?


Read other papers to get ideas to enhance this....
    How will we give hints for object textures in a mobile setting?

GET THIS RUNNING WITH UNITY OR PYBULLET!!!

Try reconstructing the individual fourier channels instead of reconstructing UV. 

Can we find a way to quickly synthsize new latent textures via fibbing?
	TODO: implement fibbing

Make a more efficient learnable image that also supports higher reoslution by making a projector function, and only analyzing the pixel coordinates that need to be analyzed. This will let us use more textures at potentially higher resolutions (or even infinite) and will allow differentiable textures in the u/v directions

ADD INTERPOLATION TO PROJECTOR! It needs to be differentiable if we want to put loss on UV textures etc.
    THOUGH....these gradients might not be excellent........maybe make the recovery resolution small? lol...idk...
    We probably want to do this on the simulated ones especially because we have ground truth labels...how would we differentiate the labels???
        Recovered UVL:
            Labels turned into one-hot-probabilities, then a weighted texture of the learned textures based on those probabilities...
            ...with an additional loss that tries to make those probabilities as close to one-hot as possible? (Or just rely on the GAN for that...)
        


POSSIBLE NEXT VERSION:
    Also learn normal maps (add onto global normals, rotated appropriately)
    Use those normals: 
        Either feed them in as just a few more channels or...
        Have a pipeline that takes albedo and normals and outputs renderings:
            Has a sky-dome that lights the object (by multiplying the albedo)
            Might also add reflections...not 100% sure how that would work though...
            Reasons: it could extract normal maps, recover lighting (make training new objects easier perhaps)
            ***All of this would allow us to relight the scene***

Don't name the Sunkist experiment 'config'!

Organize this TODO

Try getting shadow maps by subtracting average texture from individual images. 
StylegGAN3: Can this solve the flickering problems?

ADVANTAGES OVER 3D SCANS:
    Shadows, Reflections handled
    Cleaner geometry
    Unsupervised
    Can recover UV's from photos
    Combines with cyclegan in one step view-consistently
    Hopefully: Can be applied to random junk objects
    Hopefully: Can artistically style 3d objects


I have a LOT of tests I want to do...but only one computer. I need to make the lab's computers more convenient somehow....minimally my laptop should be able to mosh into them (in one step). Also they should be able to host jupyter servers and stuff. Zerotier might be the solution?

DIRECTIONS:
    Reconstuct UVL from photo
        Retexturing things in VR
        Detecting geometry of hands
        Localizing objects in a training bin
    Enhance metallic rendering
    Stylize 3d models
    Robotic learning improvements
        Both from single and multi-images
    Possibly video games? (Pinball or foozeball or chess etc)
    


Make better visulizations, slides, and names for things. Might need a new repo when I'm done...

UV RECORVERY IDEAS:
    L2/MSSIM( photo, a2b( project/concat/etc( b2a( photo ))) )     (Nice because intuitive loss, uses MSSIM instead of L2 pixelwise)
    L2( enc_b(photo), enc_a(project/concat/etc( b2a( photo ))) )  (loss in latent space)
    Add to the view consistency loss a few more recovered textures:  unproject( photos, b2a( photo ).uvl )    (maybe multiply their weights by a hyperparameter that increases over time)
    USES:
        Use differentiable rendering to position a mesh to match those UV's in the photo(s)

ALTERNATIVE TO CURRENT VIEW CONSISTENCY LOSS:
    Current consistency loss can't use MSSSIM. Current consistency loss is almost (or maybe exactly) equivalent to minimizing the distance to the average texture. MSSSIM can't do that because the textures between views mightnot be consistent...
        Can also use a perceptual loss instead of MSSSIM if we do it this way...
	But what we can do: Get the average texture, reproject it back onto the UV's, and use a MSSSIM lsos between each translation and it's recovered-texture reprojection.

CHERRY PICKING:
	Instead of using random uniform samples, perhaps choose a random sample then use other images that have common pixels for the rest of the batch.

THINGS TO CLEAN UP:
	The blender code (debugging)
	The dataset paths in ./datasets
		Referenced in other files too
	translator_tutorial.ipynb
	Renaming things such as texture consistency loss etc

Add labels to the output images...? maybe...

HOW TO GET REFLECTIONS TO WORK:
    We add the reflection channels from the blender models into the objects.
    We might want a bunch of 1x1 convolutions somewhere....not quite sure where to put them yet. Perhaps add them to the very end? 
    **If we do this, does it make sense to have cycle-consistency wth matte objects to their reflection vectors? Alphacube for example....

***An interesting extension: Enhancing the photo-to-uv part of this. If we could do that.....oh boy.....we'd have object localization down to a sciecne!
    TO DO THIS: Apply a view consistency loss between the RECOVERED FAKE UVL map from real images, and those real images!

An idea to make segmentations partially work:
    Can we use Gatys et all-esque neural style instea of just msssim+colors?

How do screen space reflections and AO work? Could I make it learned somehow?

Often robot arms will be picking up many of the *Same* objects. Can we benefit from having many objects with the same texture (and perhaps even small variants of it?)

Create a way to save config files for every stage....I want records of these on the checkpoints....I want to know what worked and what didn't on which iterations....

progressive growing:  train small then train big
    Increase by factor of sqrt(2) because 2 is too much, and as we increase by sqrt(2) twice we double the resolution
    Can we jump by larger factors if we have a loss that says the larger translations (after being downsampled) should equal the small translation outputs?
        Maybe do this until the net gets back on it's feet?
    Maybe slowly/continuously increase size over training time?
     *NOTE: When texture_multiplier is low, it *immediately* distinguishes the cube from the background. For some reason, it ignores the cube we gave it in the other 3 channels!
        I turned texture_multiplier low and turned weight decay off, but the resulting texture is *still* blueish....why?
        THe other 3 chanels dont hurt...theyre not slower or worse...but seemm unnessecary. Make them toggleable.
        Dueing progressive training, start off with low texture_multiplier (like .5) then later make it big (like 5)
            Maybe fade out UVL instead?
    Res 64 is min resolution to make network not crash
    Res 64 doesn't use the full GPU ( i bet the data loading is the bottleneck and it can't keep up with the gpu)

Try antialiasing? The sample UVL Scene could be downsampled after projection

FASTEST TRAINING METHOD (but complex)
    - Step 1: do naive image translation (with no texture, just view consistency perhaps) on low resolution for 10,000 or so gens
    - Extract texture and use as initial neural teture for step 2
    - Step 2: Start at low resolution, then do full view-consistent-translation with texture for 10,000 or so gen
    - Keep progressively increasing resolution

NEW RESEARCH IDEAS:
    **** can consistency across image zooms/translations improve general image to image translation for any domain (even cyclegan etc)???
    MSSSIM for NERF


BIG QUESTION:
    If we use 6 channels (one for originl UVL and the other the projected texture), can we avoid having to use the texture reality loss? 
        * Are both together better than either one?
    Can we do artistic style transfer? (make 3d scanned assets look like Arcane)? (HOW? We use 3 extra input channels, and still constrain only the last 3 to look like the translations)
        * We might add another channel, a single channel from a pixel-wise MLP that takes normals and returns values for lighting...

TODO:
    Make every version toggleable from config. For example, using 3 instad of 6 channels or not using textures or consistency loss. THis will be good for ablations.

***As it stands now, the generator and discriminator will have more say when there is less view consistency loss. In other words, as you zoom in, it will act more like vanilla translation. Should we scale the gen/discrim loss by this to keep them in sync?

For presentation:
  Create animations of evolution over time. Note how the new one prefers circular rings at first, and the weird artifacts.
  Create animations of the cubes moving to SHOW temporal consistency. Chart their (recovered) textures throughout the animation as well.
    Do this all in a jupyter notebook.
    Use git LFS to store the animation files and ipynb's containing said animations. Animations will be saved as a series of JPG's.
  Put the images in my downloads folder into th presentaiotn

Debug what I did to make vim start up so slowly..........oooofffff.......set up a git repo for vimrc......

Save textures as part of the training process...

NOTES:
  As we see in version 0.0.3, a large batch size is very slow...but it helps. It makes it much better. (Note large = 5, as opposed to 2. 1 is useless: there is no view consistency loss with a batch size of 1)
  The texture multiplier has a big effect! (Though to be fair it's confounded with fourier). I'm not sure why it works exactly...but I think it's because of regularization? Anyway, when it's .1 I see small changes to the floor mainly...but when it's .5....I see the L shape on the cube!!!! It's super active!
  A big difference between mine and theirs: theirs takes 6 channels as an input...but mine only takes 3 xD I want to see if I can keep it that way...it would make it great for the simulators!!! (Residual textures FTW!)
    If transformers can do residual stuff instead of concatenating because of frequency differences...why can't I? I have a theory on why that worked, as I told letitia (coffe bean channel), and I was right I think? In either case, it works! 
  A big difference between tex and no tex: the blurryness of the table. It doesn't shift!
    If we ever use mobile robots with multi rooms, many images will share 0 common UV's. In this case we have to choose the batch less randomly: choose images that overlap
  Setting the texture multiplier from .1 to .5, then from .5 to 5 had a big impact on the evolution of the algo. .5 was better than .1 (it created an L shape and drew on the cube, unlike .1 which didn't draw on the cube). But 5, learned much faster that the top of the cube is red and white than .5 did (epoch 2000 vs 10000 or so lol)

The gen_opt and dis_opt both have schedulers. Should the texture optimizer? Would that make it better?


The table blur: Why horizontal? It comes from inconsistencies from the cropping process: the images are only shifted left and right because of the rescaling and the fact that the input images are rectantular. Once we were able to know where it was (because of the textures) that problem dissapeared! This also means the textures helped!!

See if I can get away with smaller batch sizes? It might go from 6 hours to 3 hours....maybe....


TODO:
    The original code for lapro (in the archived things above this dir) is better.
    The Nerfed one, in particular, achieves *much* cleaner, smoother results than this one when used in vanilla mode.
        NOTE: This might have something to do with BILINEAR vs NEAREST (precise) in data.py? Note how the wooden background looks smoother too...
            ** precise is implemented right tho...
        NOTE: I think this happened when i started using .exr's?
    The view consistency loss helps bring it closer to that, but we really should merge the differences.
    Do some tests: when exactly did it start to look worse? (i'm not sure. maybe it's the new way we load data? We should find out...
        ...but unfortunately this takes time...looking at the pics can we predict whether it will be smooth or not from an early epoch?)

TODO:
    Create a python .rpy file to interface with the im2im: I want a simple input-output function that takes in the UVL maps (UV, Label) and outputs a translated image.
    From this, we can create animations, and create our own sample images for further testing etc.
    TODO:
        We need to simulate the augmentation scaling etc that this algorithm uses somehow, in a nice deterministic way...however the test images are made we should imitate that
        (if we want a smooth animation, the preprocessing has to be nonrandom)

I made the renderer in the gen but NOT in anything else, such as the translations.py or the discriminator trainer!
It HAS to be EVERYWHERE!
TODO:
    Make a more serious attempt. Don't rush this.
    We need image outputs for 6 channels. Why don't we start here? Have 6-channel inputs. Also, render the textures. Try all of that.
    Then, solve the NAN problem. Why are there NAN's?
   
TO PRESENT:
    Why use fourier instead of pixels?
        Fourier means that when we have gaps between the UV positions in our translated scene (like we often do; see the unprojections in the demo - there are holes everywhere) that the parts in between are also evolved. This means the UV maps being good also has a positive effect on the results; and that the textures we generate are technically resolution independent (which is pretty cool)
        TLDR: it help it learn better from incomplete information

TODO:
    Come up with a strategy to choose batches that have common UV positions so we can train more efficiently. This will be super important later on when we have more complicated scenes; and ESCPECIALLY when we have mobile robotic scenes.

RYOO TODO:
    View consistency should be called texture consistncy
    EXPERIMENTS:
        We can exclude the uarm form the view and just do a bunch of close-your-eyes then manipilate of scenes to see if this method outperformes the baseline
    Neural texture could be called latent texture (this would break convention though)

TODO: Fibbing
    Try "Fibbing" textures: using the translations, we take advantage of the fact that a single translation is less blurry than an average of them. Instead of averaging them all together, we choose a tranlsation with maximal texture coverage for a particular label. Then we choose another translation that agrees with it most (prefereably agrees vacuously; meaning it has no common texture areas). Fill that in. No averaging: just writing in pixels to the recovered texture. Keep choosing the texture that agrees most with the current recovered texture. There are many possible solutions, so its nose grows like pinnochio: it's fibbing!
    
OBSERVATIONS:
    - v0.0.8: High resolution works wonders! I had to crop it all to make it work though. This presented new problems though before v0.0.8 was reached (between 0.0.7 and 0.0.8 was the bad high res test. see the saved images). The trick to getting this to work? Make the neural texture look like the translated output.
        - To do this: I only tried this with both MSSSIM and MSE at the same time.
        - Why? Why not just MSE or just MSSSIM?
        - MSE was the first thing I tried. But the neural texture looks better when I combine it with MSSSIM. MSSSIM alone doesn't correct the colors enough to my liking though (MSE makes the colors better). MSSSIM and MSE together look good though.
        - Why does this work better? Idk. But I have my hunches: I forced it so that if you blend the neural texture of the cube into the background, then that means the actual cubes have to dissapear too! This is like the view consistency loss in a way...but that alone seemed too weak to stop it from making phantom cubes on the floor (though to be fair, I never have let it run for a long time with a higher view consistency loss weight). Also intuitively, it just makes sense to have the neural texture look like the translation. It makes the neural texture more interperatable.
    - A side note that could become a paper in of itself: MSSSIM is MUCH MUCH faster at learning neural fourier images. Maybe this could improve NERF? See the demo!!!
    - Adding a learning rate schedule to the neural texture seeeeemmmmmssss to have made it more stable....I'm not seeing NaN's anymore mid-training (yet...). 
    - Interestingly, the orange side of the alphacube texture always appears in the same place
        - Even more interestingly...this seems to have completely flipped mid-training in the high res version v0.0.8
    - The butterfly took the longest to train
    
    
FURTHER EXPERIMENTS:
    I'm having difficulty increasing the resultion (becuase of vram limitations I had to crop the image etc). And I noticed that the texture doesnt always align with the translation's cube positions...what if we add another loss to make the translated texture similar to the translation, and apply it in both directions like QVAE?
    Use MSSIM in basic ipynb
    Use MSSIM here too

