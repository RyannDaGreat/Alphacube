# Copyright (C) 2018 NVIDIA Corporation.  All rights reserved.
# Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).

# logger options

# # ryan-specific optimization options ACTUAL RUNS
max_iter: 300000            # maximum number of training iterations
batch_size: 5               # batch size
image_save_iter: 250        # How often do you want to save output images during training
image_display_iter: 250     # (How often to update train_current.png and test_current.png's) How often do you want to display output images during training
display_size: 16            # How many images do you want to display each time
snapshot_save_iter: 5000    # How often do you want to save trained models
log_iter: 10                # How often do you want to log the training stats

# # ryan-specific optimization options TESTING BREAKYNESS
# max_iter: 30                # maximum number of training iterations
# batch_size: 2               # batch size
# image_save_iter: 10         # How often do you want to save output images during training
# image_display_iter: 10      # How often do you want to display output images during training
# snapshot_save_iter: 10    # How often do you want to save trained models

# other optimization options
weight_decay: 0.0001          # weight decay
beta1: 0.5                    # Adam parameter
beta2: 0.999                  # Adam parameter
init: kaiming                 # initialization [gaussian/kaiming/xavier/orthogonal]
lr: 0.0001                    # initial learning rate lr_policy: step ### LEAVE THIS ALONE OR IT MIGHT NOT CONVERGE (even 10x is bad)
step_size: 100000             # how often to decay learning rate
gamma: 0.5                    # how much to decay learning rate
gan_w: 1                      # weight of adversarial loss
recon_x_w: 10                 # weight of image reconstruction loss
recon_s_w: 1                  # weight of style reconstruction loss
recon_c_w: 1                  # weight of content reconstruction loss
recon_x_cyc_w: 10             # weight of explicit style augmented cycle consistency loss
# label_w: 1                    # weight of label loss
# ms_ssim_a_w: 0                # weight of multi-scale structural similarity loss (a -> b)
# ms_ssim_b_w: 0                # weight of multi-scale structural similarity loss (b -> a)

# model options
gen:
  dim: 64                     # number of filters in the bottommost layer
  mlp_dim: 256                # number of filters in MLP
  # style_dim: 8                # length of style code
  activ: relu                 # activation function [relu/lrelu/prelu/selu/tanh]
  n_downsample: 2             # number of downsampling layers in content encoder #I want to set this to 3 but I run out of VRAM....
  n_res: 4                    # number of residual blocks in content encoder/decoder
  pad_type: reflect           # padding type [zero/reflect]
  # num_classes: 8              # number of classes in segmentation map
dis:
  dim: 64                     # number of filters in the bottommost layer
  norm: none                  # normalization layer [none/bn/in/ln]
  activ: lrelu                # activation function [relu/lrelu/prelu/selu/tanh]
  n_layer: 4                  # number of layers in D
  gan_type: lsgan             # GAN loss [lsgan/nsgan]
  num_scales: 3               # number of scales #ARE THESE THE THREE DISCRIMINATORS I SAW IN MUNIT?!?!?
  pad_type: reflect           # padding type [zero/reflect]

# data options
input_dim_a: 6                    # number of image channels [1/3]
input_dim_b: 3                    # number of image channels [1/3]
num_workers: 8                    # number of data loading threads

crop_image_height: 256            # random crop image of this height
crop_image_width: 256             # random crop image of this width
# Data options used during translation (after models have been trained) by translate.py.
# If these are not set, will fall back to use the data options above.
crop_image_height_translation: 256                      # random crop image of this height
crop_image_width_translation: 256                       # random crop image of this width

#GOOD RESOLUTION STAGES TO TRAIN BY:
new_size_min_a: 300               # first (randomly) resize the shortest image side to this size
new_size_max_a: 300               # first (randomly) resize the shortest image side to this size
new_size_min_b: 300               # first (randomly) resize the shortest image side to this size
new_size_max_b: 300               # first (randomly) resize the shortest image side to this size
#   64, 128, 256, 300, 320, 450
#TIMING PER ITER:
#     64: μ=0.1981049444,  σ=0.0281622
#    128: μ=0.29

data_root: ./datasets/five_items   # dataset folder location
